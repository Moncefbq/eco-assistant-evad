import requests
import json
import os  # âœ… pour lire la clÃ© depuis les variables dâ€™environnement (sÃ©curisÃ©)

# --- Configuration Hugging Face ---
HF_TOKEN = os.getenv("HF_TOKEN")  # ğŸ”’ ta clÃ© stockÃ©e dans Streamlit Cloud (ou localement)
API_URL = "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2"

# --- Fonction principale ---
def ask_model(description: str):
    headers = {"Authorization": f"Bearer {HF_TOKEN}"}
    payload = {
        "inputs": f"Tu es un assistant expert en projets Ã©cologiques. Analyse ce projet et rÃ©dige une proposition claire et dÃ©taillÃ©e : {description}",
        "parameters": {"temperature": 0.6, "max_new_tokens": 400},
    }

    try:
        response = requests.post(API_URL, headers=headers, json=payload)
        response.raise_for_status()
        result = response.json()

        # --- Extraction du texte de sortie ---
        if isinstance(result, list) and len(result) > 0 and "generated_text" in result[0]:
            message = result[0]["generated_text"]
        else:
            message = json.dumps(result, indent=2, ensure_ascii=False)

        return {
            "Titre": "Projet Ã‰cologique ProposÃ©",
            "Description": message,
            "Type": "Ã€ dÃ©terminer selon le contexte",
            "Revenus": "Estimation Ã  complÃ©ter"
        }

    except Exception as e:
        return {"error": str(e)}

# --- Simulation dâ€™enregistrement NoCoDB ---
def save_to_nocodb(data: dict):
    print("âœ… DonnÃ©es prÃªtes Ã  Ãªtre envoyÃ©es Ã  NoCoDB :")
    print(json.dumps(data, indent=2, ensure_ascii=False))
    return {"status": "success"}
